# Отток клиентов
Ситуация: из «Бета-Банка» стали уходить клиенты, каждый месяц по-немногу, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.

Задача: спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. То есть задача бинарной классификации. Нам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком.

Методы: построим модель со значением F1-меры не менее 0.59. Проверим F1-меру на тестовой выборке. Дополнительно измерим AUC-ROC, сравнивая её значение с F1-мерой.

# Итоги
На начальном этапе данные были подготовлены к дальнейшему исследованию: были устранены пропуски методом SimpleImputer, а также удалены неинформативные столбцы (имя, id, номер строки в данных). Также ко всем категориальным признакам был применен метод OHE, а ко всем количественным стандартизация методом StandardScaler.

Для определения наилучшей модели поделили данные на обучающую, валидационную и тестовую выборки. В работе были рассмотрены такие модели, как:

1) Дерево решений; 
2) Случайный лес;
 3) Логистическая регрессия.

В начале были рассмотрены модели без учета дисбаланса данных целевого признака, на этом этапе значения метрики F1 не удовлетворяли требованиям. В связи с этим для достижения нужного результата было решено применить поочерёдно ряд методов в борьбе с дисбалансом классов, а именно:

1) Взвешенные классы;
2) Увеличение выборки;
3) Уменьшение выборки;
4) Изменение порога.

Во всех случаях лидировала модель случайного леса, однако наибольшие результаты метрики F1 модель получала в случае взвешенных классов (0.648) и увеличения выборки (0.645).

На тестовых данных проверяли две модели:

1) Случайный лес со взвешенными классами (F1 - 0.64, AUC-ROC - 0.96); 
2) Случайный лес со взвешенными классами и увеличением выборки (F1 - 0.598, AUC-ROC - 0.9)

AUC-ROC много больше, чем 0.5, это значит, что наши модели не предсказывают случайно.

Итак, требуемое значение F1-меры достигнуто, наилучшая модель найдена.

# Стек
Pandas, matplotlib, numpy, seaborn, scikit-learn